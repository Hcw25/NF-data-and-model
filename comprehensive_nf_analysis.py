# -*- coding: utf-8 -*-
"""
Comprehensive Nanofiltration Membrane Machine Learning Analysis System
ç»¼åˆçº³æ»¤è†œæœºå™¨å­¦ä¹ åˆ†æç³»ç»Ÿ

Created for predicting membrane permeance and Li-Mg separation coefficient
ç”¨äºé¢„æµ‹è†œæ¸—é€æ€§å’Œé”‚é•åˆ†ç¦»ç³»æ•°

@author: ML Analysis System
"""

import os
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from datetime import datetime

# Machine Learning Libraries / æœºå™¨å­¦ä¹ åº“
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.impute import SimpleImputer

# SHAP for model interpretability / SHAPç”¨äºæ¨¡å‹å¯è§£é‡Šæ€§
import shap

# Progress tracking / è¿›åº¦è·Ÿè¸ª
from tqdm import tqdm

# Suppress warnings for cleaner output / æŠ‘åˆ¶è­¦å‘Šä»¥è·å¾—æ›´æ¸…æ´çš„è¾“å‡º
warnings.filterwarnings('ignore')

class NanofiltrationMLAnalyzer:
    """
    Comprehensive ML Analysis System for Nanofiltration Membrane Data
    çº³æ»¤è†œæ•°æ®ç»¼åˆæœºå™¨å­¦ä¹ åˆ†æç³»ç»Ÿ
    """
    
    def __init__(self, data_file='Data.xlsx', output_dir=None):
        """
        Initialize the analyzer / åˆå§‹åŒ–åˆ†æå™¨
        
        Parameters:
        - data_file: Path to the data file / æ•°æ®æ–‡ä»¶è·¯å¾„
        - output_dir: Output directory for results / ç»“æœè¾“å‡ºç›®å½•
        """
        self.data_file = data_file
        self.output_dir = output_dir or os.path.expanduser('~/Desktop/NF_Analysis_Results')
        self.create_output_directories()
        
        # Target variables / ç›®æ ‡å˜é‡
        self.target_variables = {
            'permeance': 'Permeance',
            'separation': 'Lithium - Magnesium Separation Coefficient'
        }
        
        # Results storage / ç»“æœå­˜å‚¨
        self.results = {}
        self.models = {}
        self.processed_data = {}
        
        # Model configurations / æ¨¡å‹é…ç½®
        self.n_experiments = 300  # Following example code pattern / éµå¾ªç¤ºä¾‹ä»£ç æ¨¡å¼
        
        print(f"ğŸ”¬ Nanofiltration ML Analyzer Initialized")
        print(f"ğŸ“ Output Directory: {self.output_dir}")
        print(f"ğŸ¯ Target Variables: {list(self.target_variables.keys())}")
    
    def create_output_directories(self):
        """Create organized output directories / åˆ›å»ºæœ‰ç»„ç»‡çš„è¾“å‡ºç›®å½•"""
        subdirs = ['csv_results', 'figures', 'shap_analysis', 'model_comparisons']
        
        for subdir in subdirs:
            path = os.path.join(self.output_dir, subdir)
            os.makedirs(path, exist_ok=True)
        
        print(f"ğŸ“‚ Created output directories in: {self.output_dir}")
    
    def load_and_preprocess_data(self):
        """
        Load and preprocess the nanofiltration membrane data
        åŠ è½½å’Œé¢„å¤„ç†çº³æ»¤è†œæ•°æ®
        """
        print("\nğŸ”„ Loading and preprocessing data...")
        
        # Load data / åŠ è½½æ•°æ®
        df = pd.read_excel(self.data_file)
        print(f"ğŸ“Š Original data shape: {df.shape}")
        
        # Remove the units row (first row) / ç§»é™¤å•ä½è¡Œï¼ˆç¬¬ä¸€è¡Œï¼‰
        df = df.iloc[1:].reset_index(drop=True)
        
        # Clean column names / æ¸…ç†åˆ—å
        df.columns = df.columns.str.strip()
        
        # Identify feature columns (exclude non-numeric and target columns) / è¯†åˆ«ç‰¹å¾åˆ—
        exclude_cols = ['DataNo.', 'Monomers', 'Support Membrane'] + list(self.target_variables.values())
        
        feature_cols = []
        for col in df.columns:
            if col not in exclude_cols:
                feature_cols.append(col)
        
        print(f"ğŸ¯ Feature columns identified: {len(feature_cols)}")
        print(f"ğŸ“ Features: {feature_cols}")
        
        # Process each target variable / å¤„ç†æ¯ä¸ªç›®æ ‡å˜é‡
        for target_name, target_col in self.target_variables.items():
            print(f"\nğŸ¯ Processing target: {target_name} ({target_col})")
            
            # Prepare feature and target data / å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡æ•°æ®
            X_raw = df[feature_cols].copy()
            y_raw = df[target_col].copy()
            
            # Clean and convert data / æ¸…ç†å’Œè½¬æ¢æ•°æ®
            X_clean = self._clean_features(X_raw)
            y_clean = self._clean_target(y_raw)
            
            # Remove rows with missing target values / ç§»é™¤ç›®æ ‡å€¼ç¼ºå¤±çš„è¡Œ
            valid_indices = ~pd.isna(y_clean)
            X_final = X_clean[valid_indices]
            y_final = y_clean[valid_indices]
            
            print(f"âœ… Final dataset shape: X={X_final.shape}, y={y_final.shape}")
            
            # Store processed data / å­˜å‚¨å¤„ç†åçš„æ•°æ®
            self.processed_data[target_name] = {
                'X': X_final,
                'y': y_final,
                'feature_names': feature_cols,
                'original_size': len(df),
                'final_size': len(y_final)
            }
    
    def _clean_features(self, X):
        """Clean feature data / æ¸…ç†ç‰¹å¾æ•°æ®"""
        X_clean = X.copy()
        
        for col in X_clean.columns:
            # Replace '/' and other non-numeric values with NaN / å°†'/'ç­‰éæ•°å€¼æ›¿æ¢ä¸ºNaN
            X_clean[col] = X_clean[col].replace(['/', '\\', 'NaN', 'nan', ''], np.nan)
            
            # Convert to numeric / è½¬æ¢ä¸ºæ•°å€¼å‹
            X_clean[col] = pd.to_numeric(X_clean[col], errors='coerce')
        
        # Impute missing values using median / ä½¿ç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼
        imputer = SimpleImputer(strategy='median')
        X_clean = pd.DataFrame(
            imputer.fit_transform(X_clean),
            columns=X_clean.columns,
            index=X_clean.index
        )
        
        return X_clean
    
    def _clean_target(self, y):
        """Clean target data / æ¸…ç†ç›®æ ‡æ•°æ®"""
        y_clean = y.copy()
        
        # Replace non-numeric values with NaN / å°†éæ•°å€¼æ›¿æ¢ä¸ºNaN
        y_clean = y_clean.replace(['/', '\\', 'NaN', 'nan', ''], np.nan)
        
        # Convert to numeric / è½¬æ¢ä¸ºæ•°å€¼å‹
        y_clean = pd.to_numeric(y_clean, errors='coerce')
        
        return y_clean
    
    def run_comprehensive_analysis(self):
        """
        Run comprehensive ML analysis for all targets and models
        å¯¹æ‰€æœ‰ç›®æ ‡å’Œæ¨¡å‹è¿è¡Œç»¼åˆMLåˆ†æ
        """
        print("\nğŸš€ Starting Comprehensive ML Analysis...")
        
        # Load and preprocess data / åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        self.load_and_preprocess_data()
        
        # Analyze each target variable / åˆ†ææ¯ä¸ªç›®æ ‡å˜é‡
        for target_name in self.target_variables.keys():
            print(f"\n{'='*60}")
            print(f"ğŸ¯ ANALYZING TARGET: {target_name.upper()}")
            print(f"{'='*60}")
            
            self.analyze_target(target_name)
        
        # Generate comparative analysis / ç”Ÿæˆæ¯”è¾ƒåˆ†æ
        self.generate_model_comparisons()
        
        print("\nğŸ‰ Analysis Complete!")
        print(f"ğŸ“ Results saved to: {self.output_dir}")
    
    def analyze_target(self, target_name):
        """
        Analyze a specific target variable with all models
        ä½¿ç”¨æ‰€æœ‰æ¨¡å‹åˆ†æç‰¹å®šç›®æ ‡å˜é‡
        """
        data = self.processed_data[target_name]
        X, y = data['X'], data['y']
        feature_names = data['feature_names']
        
        # Initialize results storage for this target / åˆå§‹åŒ–æ­¤ç›®æ ‡çš„ç»“æœå­˜å‚¨
        self.results[target_name] = {}
        self.models[target_name] = {}
        
        # Define models / å®šä¹‰æ¨¡å‹
        models_config = {
            'XGBoost': {
                'model_class': xgb.XGBRegressor,
                'params': {
                    'n_estimators': 180,
                    'learning_rate': 0.06,
                    'max_depth': 6,
                    'objective': 'reg:squarederror',
                    'random_state': 7
                }
            },
            'RandomForest': {
                'model_class': RandomForestRegressor,
                'params': {
                    'n_estimators': 180,
                    'max_depth': 10,
                    'random_state': 7,
                    'n_jobs': -1
                }
            },
            'NeuralNetwork': {
                'model_class': MLPRegressor,
                'params': {
                    'hidden_layer_sizes': (100, 50),
                    'max_iter': 1000,
                    'random_state': 7,
                    'alpha': 0.001
                }
            }
        }
        
        # Analyze each model / åˆ†ææ¯ä¸ªæ¨¡å‹
        for model_name, config in models_config.items():
            print(f"\nğŸ” Training {model_name} model...")
            self.train_and_evaluate_model(target_name, model_name, config, X, y, feature_names)
        
        # Generate SHAP analysis for best model / ä¸ºæœ€ä½³æ¨¡å‹ç”ŸæˆSHAPåˆ†æ
        self.generate_shap_analysis(target_name, X, feature_names)
    
    def train_and_evaluate_model(self, target_name, model_name, config, X, y, feature_names):
        """
        Train and evaluate a model with cross-validation
        ä½¿ç”¨äº¤å‰éªŒè¯è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
        """
        # Storage for multiple experiments / å¤šæ¬¡å®éªŒçš„å­˜å‚¨
        experiment_results = []
        feature_importances = []
        best_model = None
        best_score = -np.inf
        
        # Standardize features for neural networks / ä¸ºç¥ç»ç½‘ç»œæ ‡å‡†åŒ–ç‰¹å¾
        if model_name == 'NeuralNetwork':
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
        else:
            X_scaled = X
            scaler = None
        
        print(f"ğŸ”„ Running {self.n_experiments} experiments...")
        
        # Multiple experiments for robust evaluation / å¤šæ¬¡å®éªŒä»¥è¿›è¡Œç¨³å¥è¯„ä¼°
        for i in tqdm(range(self.n_experiments), desc=f"Training {model_name}"):
            try:
                # Split data / åˆ†å‰²æ•°æ®
                X_train, X_test, y_train, y_test = train_test_split(
                    X_scaled, y, test_size=0.2, random_state=i
                )
                
                # Create and train model / åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
                model = config['model_class'](**config['params'])
                model.fit(X_train, y_train)
                
                # Make predictions / è¿›è¡Œé¢„æµ‹
                y_pred = model.predict(X_test)
                
                # Calculate metrics / è®¡ç®—æŒ‡æ ‡
                pearson_r = stats.pearsonr(y_test, y_pred)[0]
                r2 = r2_score(y_test, y_pred)
                rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                mae = mean_absolute_error(y_test, y_pred)
                
                # Store results / å­˜å‚¨ç»“æœ
                result = {
                    'experiment': i,
                    'pearson_r': pearson_r,
                    'r2': r2,
                    'rmse': rmse,
                    'mae': mae
                }
                
                # Extract feature importance / æå–ç‰¹å¾é‡è¦æ€§
                if hasattr(model, 'feature_importances_'):
                    importance = model.feature_importances_
                    result.update({f'importance_{j}': importance[j] for j in range(len(importance))})
                    feature_importances.append(importance)
                
                experiment_results.append(result)
                
                # Track best model / è·Ÿè¸ªæœ€ä½³æ¨¡å‹
                if r2 > best_score:
                    best_score = r2
                    best_model = model
                    
            except Exception as e:
                print(f"âš ï¸ Experiment {i} failed: {str(e)}")
                continue
        
        # Store results / å­˜å‚¨ç»“æœ
        self.results[target_name][model_name] = {
            'experiments': experiment_results,
            'feature_names': feature_names,
            'scaler': scaler
        }
        
        self.models[target_name][model_name] = best_model
        
        # Calculate summary statistics / è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        self.calculate_model_statistics(target_name, model_name)
        
        # Save detailed results to CSV / å°†è¯¦ç»†ç»“æœä¿å­˜ä¸ºCSV
        self.save_model_results_csv(target_name, model_name)
        
        # Generate visualizations / ç”Ÿæˆå¯è§†åŒ–
        self.generate_model_visualizations(target_name, model_name)
    
    def calculate_model_statistics(self, target_name, model_name):
        """Calculate summary statistics for model performance / è®¡ç®—æ¨¡å‹æ€§èƒ½çš„æ±‡æ€»ç»Ÿè®¡"""
        results = self.results[target_name][model_name]['experiments']
        df_results = pd.DataFrame(results)
        
        # Calculate mean and std for each metric / è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„å‡å€¼å’Œæ ‡å‡†å·®
        metrics = ['pearson_r', 'r2', 'rmse', 'mae']
        stats_summary = {}
        
        for metric in metrics:
            stats_summary[f'{metric}_mean'] = df_results[metric].mean()
            stats_summary[f'{metric}_std'] = df_results[metric].std()
        
        # Calculate feature importance statistics / è®¡ç®—ç‰¹å¾é‡è¦æ€§ç»Ÿè®¡
        importance_cols = [col for col in df_results.columns if col.startswith('importance_')]
        if importance_cols:
            feature_importance_stats = {}
            for i, feature_name in enumerate(self.results[target_name][model_name]['feature_names']):
                importance_col = f'importance_{i}'
                if importance_col in df_results.columns:
                    feature_importance_stats[feature_name] = {
                        'mean': df_results[importance_col].mean(),
                        'std': df_results[importance_col].std()
                    }
            
            stats_summary['feature_importance'] = feature_importance_stats
        
        self.results[target_name][model_name]['statistics'] = stats_summary
        
        # Print summary / æ‰“å°æ‘˜è¦
        print(f"\nğŸ“Š {model_name} Results Summary:")
        for metric in metrics:
            mean_val = stats_summary[f'{metric}_mean']
            std_val = stats_summary[f'{metric}_std']
            print(f"   {metric.upper()}: {mean_val:.4f} Â± {std_val:.4f}")
    
    def save_model_results_csv(self, target_name, model_name):
        """Save detailed model results to CSV / å°†è¯¦ç»†æ¨¡å‹ç»“æœä¿å­˜ä¸ºCSV"""
        results = self.results[target_name][model_name]['experiments']
        df_results = pd.DataFrame(results)
        
        # Save experiment results / ä¿å­˜å®éªŒç»“æœ
        filename = f"{target_name}_{model_name}_detailed_results.csv"
        filepath = os.path.join(self.output_dir, 'csv_results', filename)
        df_results.to_csv(filepath, index=False, encoding='utf-8')
        
        # Save summary statistics / ä¿å­˜æ±‡æ€»ç»Ÿè®¡
        stats = self.results[target_name][model_name]['statistics']
        
        summary_data = []
        metrics = ['pearson_r', 'r2', 'rmse', 'mae']
        for metric in metrics:
            summary_data.append({
                'Metric': metric.upper(),
                'Mean': stats[f'{metric}_mean'],
                'Std': stats[f'{metric}_std']
            })
        
        df_summary = pd.DataFrame(summary_data)
        summary_filename = f"{target_name}_{model_name}_performance_summary.csv"
        summary_filepath = os.path.join(self.output_dir, 'csv_results', summary_filename)
        df_summary.to_csv(summary_filepath, index=False, encoding='utf-8')
        
        # Save feature importance if available / å¦‚æœå¯ç”¨ï¼Œä¿å­˜ç‰¹å¾é‡è¦æ€§
        if 'feature_importance' in stats:
            importance_data = []
            for feature, imp_stats in stats['feature_importance'].items():
                importance_data.append({
                    'Feature': feature,
                    'Importance_Mean': imp_stats['mean'],
                    'Importance_Std': imp_stats['std']
                })
            
            if importance_data:
                df_importance = pd.DataFrame(importance_data)
                df_importance = df_importance.sort_values('Importance_Mean', ascending=False)
                
                importance_filename = f"{target_name}_{model_name}_feature_importance.csv"
                importance_filepath = os.path.join(self.output_dir, 'csv_results', importance_filename)
                df_importance.to_csv(importance_filepath, index=False, encoding='utf-8')
        
        print(f"ğŸ’¾ Saved results to: {filepath}")
    
    def generate_model_visualizations(self, target_name, model_name):
        """Generate visualizations for model performance / ä¸ºæ¨¡å‹æ€§èƒ½ç”Ÿæˆå¯è§†åŒ–"""
        
        # Set up plotting style / è®¾ç½®ç»˜å›¾æ ·å¼
        plt.style.use('default')
        sns.set_palette("husl")
        
        # Font settings for Chinese and English / ä¸­è‹±æ–‡å­—ä½“è®¾ç½®
        plt.rcParams['font.size'] = 12
        plt.rcParams['axes.titlesize'] = 14
        plt.rcParams['axes.labelsize'] = 12
        plt.rcParams['figure.dpi'] = 100
        
        results = self.results[target_name][model_name]['experiments']
        df_results = pd.DataFrame(results)
        
        # 1. Performance metrics distribution / æ€§èƒ½æŒ‡æ ‡åˆ†å¸ƒ
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'{model_name} - {target_name.title()} Performance Distribution\n'
                    f'{model_name} - {target_name.title()}æ€§èƒ½åˆ†å¸ƒ', fontsize=16, fontweight='bold')
        
        metrics = ['pearson_r', 'r2', 'rmse', 'mae']
        metric_labels = ['Pearson Correlation', 'RÂ² Score', 'RMSE', 'MAE']
        
        for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):
            ax = axes[idx//2, idx%2]
            ax.hist(df_results[metric], bins=30, alpha=0.7, edgecolor='black')
            ax.set_title(f'{label} Distribution')
            ax.set_xlabel(label)
            ax.set_ylabel('Frequency / é¢‘ç‡')
            ax.grid(True, alpha=0.3)
            
            # Add statistics text / æ·»åŠ ç»Ÿè®¡æ–‡æœ¬
            mean_val = df_results[metric].mean()
            std_val = df_results[metric].std()
            ax.axvline(mean_val, color='red', linestyle='--', linewidth=2)
            ax.text(0.05, 0.95, f'Mean: {mean_val:.4f}\nStd: {std_val:.4f}', 
                   transform=ax.transAxes, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        plt.tight_layout()
        filename = f"{target_name}_{model_name}_performance_distribution.png"
        filepath = os.path.join(self.output_dir, 'figures', filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Feature importance visualization (if available) / ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        stats = self.results[target_name][model_name]['statistics']
        if 'feature_importance' in stats:
            self.plot_feature_importance(target_name, model_name, stats['feature_importance'])
        
        # 3. Model prediction scatter plot / æ¨¡å‹é¢„æµ‹æ•£ç‚¹å›¾
        self.plot_prediction_scatter(target_name, model_name)
    
    def plot_feature_importance(self, target_name, model_name, feature_importance):
        """Plot feature importance / ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
        
        # Prepare data for plotting / å‡†å¤‡ç»˜å›¾æ•°æ®
        features = list(feature_importance.keys())
        importance_means = [feature_importance[f]['mean'] for f in features]
        importance_stds = [feature_importance[f]['std'] for f in features]
        
        # Sort by importance / æŒ‰é‡è¦æ€§æ’åº
        sorted_indices = np.argsort(importance_means)[::-1]
        features_sorted = [features[i] for i in sorted_indices]
        means_sorted = [importance_means[i] for i in sorted_indices]
        stds_sorted = [importance_stds[i] for i in sorted_indices]
        
        # Create plot / åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(12, 8))
        bars = plt.bar(range(len(features_sorted)), means_sorted, 
                      yerr=stds_sorted, capsize=5, alpha=0.7)
        
        plt.title(f'{model_name} - {target_name.title()} Feature Importance\n'
                 f'{model_name} - {target_name.title()}ç‰¹å¾é‡è¦æ€§', fontsize=16, fontweight='bold')
        plt.xlabel('Features / ç‰¹å¾', fontsize=12)
        plt.ylabel('Importance / é‡è¦æ€§', fontsize=12)
        
        # Rotate x-axis labels for better readability / æ—‹è½¬xè½´æ ‡ç­¾ä»¥æé«˜å¯è¯»æ€§
        plt.xticks(range(len(features_sorted)), features_sorted, rotation=45, ha='right')
        plt.grid(True, alpha=0.3)
        
        # Add value labels on bars / åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (mean, std) in enumerate(zip(means_sorted, stds_sorted)):
            plt.text(i, mean + std, f'{mean:.3f}', ha='center', va='bottom', fontsize=8)
        
        plt.tight_layout()
        filename = f"{target_name}_{model_name}_feature_importance.png"
        filepath = os.path.join(self.output_dir, 'figures', filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_prediction_scatter(self, target_name, model_name):
        """Create prediction vs actual scatter plot / åˆ›å»ºé¢„æµ‹ä¸å®é™…çš„æ•£ç‚¹å›¾"""
        
        # Get best model and data / è·å–æœ€ä½³æ¨¡å‹å’Œæ•°æ®
        model = self.models[target_name][model_name]
        data = self.processed_data[target_name]
        X, y = data['X'], data['y']
        
        # Apply scaling if needed / å¦‚æœéœ€è¦åº”ç”¨ç¼©æ”¾
        scaler = self.results[target_name][model_name].get('scaler')
        if scaler:
            X_scaled = scaler.transform(X)
        else:
            X_scaled = X
        
        # Make predictions on full dataset / å¯¹å®Œæ•´æ•°æ®é›†è¿›è¡Œé¢„æµ‹
        y_pred = model.predict(X_scaled)
        
        # Calculate metrics / è®¡ç®—æŒ‡æ ‡
        pearson_r = stats.pearsonr(y, y_pred)[0]
        r2 = r2_score(y, y_pred)
        rmse = np.sqrt(mean_squared_error(y, y_pred))
        
        # Create plot / åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(10, 8))
        plt.scatter(y, y_pred, alpha=0.6, s=50)
        
        # Perfect prediction line / å®Œç¾é¢„æµ‹çº¿
        min_val = min(y.min(), y_pred.min())
        max_val = max(y.max(), y_pred.max())
        plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
        
        plt.title(f'{model_name} - {target_name.title()} Prediction vs Actual\n'
                 f'{model_name} - {target_name.title()}é¢„æµ‹å€¼ä¸å®é™…å€¼', fontsize=16, fontweight='bold')
        plt.xlabel('Actual Values / å®é™…å€¼', fontsize=12)
        plt.ylabel('Predicted Values / é¢„æµ‹å€¼', fontsize=12)
        
        # Add statistics text / æ·»åŠ ç»Ÿè®¡æ–‡æœ¬
        stats_text = f'Pearson r: {pearson_r:.4f}\nRÂ²: {r2:.4f}\nRMSE: {rmse:.4f}'
        plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, 
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.tight_layout()
        
        filename = f"{target_name}_{model_name}_prediction_scatter.png"
        filepath = os.path.join(self.output_dir, 'figures', filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_shap_analysis(self, target_name, X, feature_names):
        """Generate SHAP analysis for the best performing model / ä¸ºè¡¨ç°æœ€ä½³çš„æ¨¡å‹ç”ŸæˆSHAPåˆ†æ"""
        
        print(f"ğŸ” Generating SHAP analysis for {target_name}...")
        
        # Find best model based on RÂ² score / åŸºäºRÂ²åˆ†æ•°æ‰¾åˆ°æœ€ä½³æ¨¡å‹
        best_model_name = None
        best_r2 = -np.inf
        
        for model_name in self.results[target_name].keys():
            stats = self.results[target_name][model_name]['statistics']
            r2_mean = stats['r2_mean']
            if r2_mean > best_r2:
                best_r2 = r2_mean
                best_model_name = model_name
        
        if best_model_name is None:
            print("âš ï¸ No valid model found for SHAP analysis")
            return
        
        print(f"ğŸ† Best model: {best_model_name} (RÂ² = {best_r2:.4f})")
        
        # Get best model and prepare data / è·å–æœ€ä½³æ¨¡å‹å¹¶å‡†å¤‡æ•°æ®
        model = self.models[target_name][best_model_name]
        scaler = self.results[target_name][best_model_name].get('scaler')
        
        if scaler:
            X_scaled = scaler.transform(X)
        else:
            X_scaled = X
        
        try:
            # Create SHAP explainer / åˆ›å»ºSHAPè§£é‡Šå™¨
            if best_model_name == 'XGBoost':
                explainer = shap.Explainer(model)
                shap_values = explainer(X_scaled)
            elif best_model_name == 'RandomForest':
                # Use TreeExplainer for RandomForest with a sample of background data
                background_data = X_scaled[:min(100, len(X_scaled))]
                explainer = shap.TreeExplainer(model, background_data)
                shap_values = explainer.shap_values(X_scaled)
                # Convert to SHAP values object for consistency
                shap_values = shap.Explanation(values=shap_values, feature_names=feature_names)
            else:  # Neural Network
                explainer = shap.Explainer(model, X_scaled[:min(100, len(X_scaled))])
                shap_values = explainer(X_scaled)
            
            # Save SHAP values to CSV / å°†SHAPå€¼ä¿å­˜ä¸ºCSV
            shap_df = pd.DataFrame(shap_values.values, columns=feature_names)
            shap_filename = f"{target_name}_shap_values.csv"
            shap_filepath = os.path.join(self.output_dir, 'shap_analysis', shap_filename)
            shap_df.to_csv(shap_filepath, index=False, encoding='utf-8')
            
            # Generate SHAP visualizations / ç”ŸæˆSHAPå¯è§†åŒ–
            self.create_shap_visualizations(target_name, best_model_name, shap_values, feature_names)
            
            print(f"ğŸ’¾ SHAP analysis saved for {target_name}")
            
        except Exception as e:
            print(f"âš ï¸ SHAP analysis failed for {target_name}: {str(e)}")
    
    def create_shap_visualizations(self, target_name, model_name, shap_values, feature_names):
        """Create SHAP visualizations / åˆ›å»ºSHAPå¯è§†åŒ–"""
        
        # 1. Feature importance plot / ç‰¹å¾é‡è¦æ€§å›¾
        plt.figure(figsize=(10, 8))
        shap.summary_plot(shap_values, feature_names=feature_names, plot_type="bar", show=False)
        plt.title(f'SHAP Feature Importance - {model_name} ({target_name.title()})\n'
                 f'SHAPç‰¹å¾é‡è¦æ€§ - {model_name} ({target_name.title()})', fontsize=14)
        plt.tight_layout()
        
        filename = f"{target_name}_shap_feature_importance.png"
        filepath = os.path.join(self.output_dir, 'shap_analysis', filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Summary plot / æ±‡æ€»å›¾
        plt.figure(figsize=(10, 8))
        shap.summary_plot(shap_values, feature_names=feature_names, show=False)
        plt.title(f'SHAP Summary Plot - {model_name} ({target_name.title()})\n'
                 f'SHAPæ±‡æ€»å›¾ - {model_name} ({target_name.title()})', fontsize=14)
        plt.tight_layout()
        
        filename = f"{target_name}_shap_summary.png"
        filepath = os.path.join(self.output_dir, 'shap_analysis', filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_model_comparisons(self):
        """Generate comprehensive model comparison visualizations / ç”Ÿæˆç»¼åˆæ¨¡å‹æ¯”è¾ƒå¯è§†åŒ–"""
        
        print("\nğŸ“Š Generating model comparisons...")
        
        for target_name in self.target_variables.keys():
            self.create_target_model_comparison(target_name)
        
        # Create overall comparison / åˆ›å»ºæ€»ä½“æ¯”è¾ƒ
        self.create_overall_comparison()
    
    def create_target_model_comparison(self, target_name):
        """Create model comparison for a specific target / ä¸ºç‰¹å®šç›®æ ‡åˆ›å»ºæ¨¡å‹æ¯”è¾ƒ"""
        
        models_in_target = list(self.results[target_name].keys())
        metrics = ['pearson_r', 'r2', 'rmse', 'mae']
        
        # Prepare comparison data / å‡†å¤‡æ¯”è¾ƒæ•°æ®
        comparison_data = []
        for model_name in models_in_target:
            stats = self.results[target_name][model_name]['statistics']
            row = {'Model': model_name}
            for metric in metrics:
                row[f'{metric}_mean'] = stats[f'{metric}_mean']
                row[f'{metric}_std'] = stats[f'{metric}_std']
            comparison_data.append(row)
        
        df_comparison = pd.DataFrame(comparison_data)
        
        # Save comparison CSV / ä¿å­˜æ¯”è¾ƒCSV
        filename = f"{target_name}_model_comparison.csv"
        filepath = os.path.join(self.output_dir, 'model_comparisons', filename)
        df_comparison.to_csv(filepath, index=False, encoding='utf-8')
        
        # Create comparison visualization / åˆ›å»ºæ¯”è¾ƒå¯è§†åŒ–
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'Model Comparison - {target_name.title()}\n'
                    f'æ¨¡å‹æ¯”è¾ƒ - {target_name.title()}', fontsize=16, fontweight='bold')
        
        metric_labels = ['Pearson Correlation', 'RÂ² Score', 'RMSE', 'MAE']
        
        for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):
            ax = axes[idx//2, idx%2]
            
            means = df_comparison[f'{metric}_mean']
            stds = df_comparison[f'{metric}_std']
            
            bars = ax.bar(df_comparison['Model'], means, yerr=stds, capsize=5, alpha=0.7)
            ax.set_title(f'{label} Comparison')
            ax.set_ylabel(label)
            ax.grid(True, alpha=0.3)
            
            # Add value labels / æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (mean, std) in enumerate(zip(means, stds)):
                ax.text(i, mean + std, f'{mean:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        filename = f"{target_name}_model_comparison.png"
        filepath = os.path.join(self.output_dir, 'model_comparisons', filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
    
    def create_overall_comparison(self):
        """Create overall comparison across all targets and models / åˆ›å»ºæ‰€æœ‰ç›®æ ‡å’Œæ¨¡å‹çš„æ€»ä½“æ¯”è¾ƒ"""
        
        # Prepare overall comparison data / å‡†å¤‡æ€»ä½“æ¯”è¾ƒæ•°æ®
        overall_data = []
        
        for target_name in self.target_variables.keys():
            for model_name in self.results[target_name].keys():
                stats = self.results[target_name][model_name]['statistics']
                row = {
                    'Target': target_name.title(),
                    'Model': model_name,
                    'Pearson_R': stats['pearson_r_mean'],
                    'R2_Score': stats['r2_mean'],
                    'RMSE': stats['rmse_mean'],
                    'MAE': stats['mae_mean']
                }
                overall_data.append(row)
        
        df_overall = pd.DataFrame(overall_data)
        
        # Save overall comparison / ä¿å­˜æ€»ä½“æ¯”è¾ƒ
        filepath = os.path.join(self.output_dir, 'model_comparisons', 'overall_comparison.csv')
        df_overall.to_csv(filepath, index=False, encoding='utf-8')
        
        # Create heatmap visualization / åˆ›å»ºçƒ­å›¾å¯è§†åŒ–
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Overall Model Performance Heatmap\næ€»ä½“æ¨¡å‹æ€§èƒ½çƒ­å›¾', fontsize=16, fontweight='bold')
        
        metrics = ['Pearson_R', 'R2_Score', 'RMSE', 'MAE']
        metric_labels = ['Pearson Correlation', 'RÂ² Score', 'RMSE', 'MAE']
        
        for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):
            ax = axes[idx//2, idx%2]
            
            # Create pivot table for heatmap / ä¸ºçƒ­å›¾åˆ›å»ºæ•°æ®é€è§†è¡¨
            pivot_data = df_overall.pivot(index='Target', columns='Model', values=metric)
            
            # Create heatmap / åˆ›å»ºçƒ­å›¾
            sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='viridis', ax=ax)
            ax.set_title(f'{label} Heatmap')
        
        plt.tight_layout()
        filepath = os.path.join(self.output_dir, 'model_comparisons', 'overall_heatmap.png')
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        
        print("ğŸ“Š Model comparisons complete!")


def main():
    """
    Main function to run the comprehensive nanofiltration analysis
    è¿è¡Œç»¼åˆçº³æ»¤åˆ†æçš„ä¸»å‡½æ•°
    """
    print("ğŸŒŸ Starting Comprehensive Nanofiltration Membrane ML Analysis")
    print("ğŸŒŸ å¼€å§‹ç»¼åˆçº³æ»¤è†œæœºå™¨å­¦ä¹ åˆ†æ")
    
    # Initialize analyzer / åˆå§‹åŒ–åˆ†æå™¨
    analyzer = NanofiltrationMLAnalyzer(data_file='Data.xlsx')
    
    # Run comprehensive analysis / è¿è¡Œç»¼åˆåˆ†æ
    analyzer.run_comprehensive_analysis()
    
    print("\nâœ… Analysis completed successfully!")
    print("âœ… åˆ†ææˆåŠŸå®Œæˆï¼")
    print(f"ğŸ“ Check results at: {analyzer.output_dir}")
    print(f"ğŸ“ è¯·æŸ¥çœ‹ç»“æœï¼š{analyzer.output_dir}")


if __name__ == "__main__":
    main()